<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ExampleSite</title>
    <link>https://www.arunneru.com/</link>
    <description>Recent content on ExampleSite</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 24 Jul 2022 15:23:28 +0100</lastBuildDate><atom:link href="https://www.arunneru.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Principle Component Analysis: An Overview</title>
      <link>https://www.arunneru.com/posts/first/</link>
      <pubDate>Sun, 24 Jul 2022 15:23:28 +0100</pubDate>
      
      <guid>https://www.arunneru.com/posts/first/</guid>
      <description>PCA is one of the most widely used methods in data science/machine learning. It is mainly used for reducing the dimension of a high dimensional data that comes handy when extracting important features relevant for further data analysis/machine learning algorithms. For example, in linear regression not having correlation betweeen principle components alleviate the bit of headache when dealing with original variables that are often correlated and give inconsistent p-values depending on which order they were included in estimating the coefficients, not to mention the reduced computational cost from dealing with a few relevant principle components as opposed to the full-extent of the original variables.</description>
    </item>
    
    <item>
      <title>Principle Component Analysis: An Overview</title>
      <link>https://www.arunneru.com/posts/pca/first/</link>
      <pubDate>Sun, 24 Jul 2022 15:23:28 +0100</pubDate>
      
      <guid>https://www.arunneru.com/posts/pca/first/</guid>
      <description>PCA is one of the most widely used methods in data science/machine learning. It is mainly used for reducing the dimension of a high dimensional data that comes handy when extracting important features relevant for further data analysis/machine learning algorithms. For example, in linear regression not having correlation betweeen principle components alleviate the bit of headache when dealing with original variables that are often correlated and give inconsistent p-values depending on which order they were included in estimating the coefficients, not to mention the reduced computational cost from dealing with a few relevant principle components as opposed to the full-extent of the original variables.</description>
    </item>
    
  </channel>
</rss>
